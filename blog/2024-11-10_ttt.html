<!DOCTYPE html>
<html>

<head>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="../style.css">
<title>learning to (learn at test time): transformers without quadratic attention</title>

</head>

<body>
<script type="text/javascript">
MathJax.Hub.Config({
	"HTML-CSS": { scale: 100},
});
</script>

<h3>
<a href="../blog.html">blog</a>
| <a href="../index.html">about</a>
</h3>

<h1>learning to (learn at test time): transformers without quadratic attention</h1>
<h3>Nov 10, 2024</h3>

<p>
The recent paper
<a href="https://arxiv.org/abs/2407.04620">
"Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
</a>
introduces <i>test-time training (TTT)</i>,
an exciting new approach to sequence modeling.
The authors present the TTT model as a recurrent neural network (RNN)
capable of achieving transformer-level quality.
In this blog post,
I motivate the TTT model from the other direction &mdash;
as a transformer with RNN-like linear complexity in the sequence length.
Towards this end,
I give an alternate formulation of TTT
as an approximation to the transformer's quadratic attention mechanism.
Apart from providing fresh intuition for TTT,
this perspective suggests interesting possibilities
such as initializing TTT models with pre-trained transformer weights.
</p>

<h2>approximating attention with test-time training</h2>

<p>
For simplicity, let us first consider attention with a single query vector.
</p>

<p>
<i>Single-query attention</i> has the type signature \( o = a(q, K, V) \), where,
<ul>
<li>\( q \in \mathbb{R}^{d_k} \) is the <i>query</i> vector,</li>
<li>\( K = [k_1, \dots, k_T], k_i \in \mathbb{R}^{d_k}, \) is the list of <i>key</i> vectors,</li>
<li>\( V = [v_1, \dots, v_T], v_i \in \mathbb{R}^{d_v}, \) is the list of <i>value</i> vectors, and,</li>
<li>\( o \in \mathbb{R}^{d_v} \) is the <i>output</i> vector.</li>
</ul>
</p>

<p>
The standard <i>scaled-dot-product-attention</i> (\( a^{\text{SDPA}} \)) is computed as follows:
<ol>
<li>\( w = \text{softmax}\left(\left[\frac{q \cdot k_1}{\sqrt{d_k}}, \dots, \frac{q \cdot k_T}{\sqrt{d_k}}\right]\right) \)</li>
<li>\( o = \sum_{i=1}^t w_i v_i \)</li>
</ol>
</p>

<p>
Below is an intuitive breakdown of the two steps:
<ol>
<li>Soft-select a key \( k_i \) most <i>similar</i> to the query \( q \),</li>
<li>Return the associated value \( v_i \).</li>
</ol>
</p>

<p>
This is reminiscent of applying a <i>nearest-neighbor model</i> with
train set \( \{(k_1, v_1), \dots, (k_T, v_T)\}, \) and
test input \( q \).
<b>What if we instead use a parametric model such as a neural network?</b>
This would look like below:
<ol>
<li>Train a neural network \( f_\theta \) to predict \( v_i \) for input \( k_i \), and,</li>
<li>Return the prediction of the network on \( q \).</li>
</ol>
</p>

<p>
This suggests \( a^{\text{TTT}} \) as a substitute for \( a^{\text{SDPA}} \),
computed as follows:
<ol>
<li>\( \theta = \arg \min_\theta \sum_{i=1}^T \Vert f_\theta(k_i) - v_i \Vert_2^2 \)</li>
<li>\( o = f_\theta(q) \). </li>
</ol>
</p>

<p>
Step 1 above trains the network \( f_\theta \) with an \( L_2 \) loss.
Note that this training happens within the lifetime of a single attention call.
The attention call can be part of an <i>outer model</i>, such as a transformer.
Inferencing the outer model at test-time would still require training \( f_\theta \)
within the attention call, hence the name "test-time training".
</p>


<h2>how does TTT achieve linear scaling?</h2>

<p>
So far, we train \( f_\theta \) on all key-value pairs
for every single-query attention call.
In this setting, \( a^{\text{TTT}} \) is at least linear in the sequence length,
no better than \( a^{\text{SDPA}} \).
To see how TTT can be more efficient,
we need to consider how single-query attention
is invoked inside the over-arching self-attention computation.
</p>

<p>
In <i>self-attention</i>, there is a query vector,
and an associated output vector,
for each key-value pair.
The type signature is \( O = A(Q, K, V)\), where,
<ul>
<li>\( Q = [q_1, \dots, q_T]\), \( q_i \in \mathbb{R}^{d_k} \) is the list of query vectors,</li>
<li>\( K = [k_1, \dots, k_T]\), \( k_i \in \mathbb{R}^{d_k} \) is the list of key vectors,</li>
<li>\( V = [v_1, \dots, v_T]\), \( v_i \in \mathbb{R}^{d_v} \) is the list of value vectors, and,</li>
<li>\( O = [o_1, \dots, o_T]\), \( o_i \in \mathbb{R}^{d_v} \) is the list of output vectors.</li>
</ul>
The relation to single-query attention is given by:
\(o_t = a(q_t, [k_1, \dots, k_t], [v_1, \dots, v_t])\).
The dependence of \( o_t \) on \( (k_i, v_i) \) only for \( i \le t \)
makes this kind of self-attention <i>causal</i>.
</p>

<p>
\(A^{\text{SDPA}} \) is quadratic in the sequence length,
as each \( o_t \) is linear in \( t\),
and no further optimization is possible.
\(A^{\text{TTT}} \) avoids this quadratic dependence
by reusing the learnt neural network from the previous timestep,
instead of training a fresh one from scratch.
For example, if we use one step of gradient descent
on every pair \( (k_t, v_t) \) with learning rate \( \eta\),
\(A^{\text{TTT}} \) can be computed as below:
</p>

<p>
For \( t=1, \dots, T\):
<ol>
<li>\( \theta_t = \theta_{t-1} - \eta \nabla_{\theta_t} \Vert f_{\theta_t}(k_t) - v_t \Vert_2^2\)</li>
<li>\( o_t = f_\theta(q_t)\)</li>
</ol>
This is linear in the sequence length \( T\).
</p>



<h2>epilogue</h2>

<p>
What I have presented is only one way to
gain an introductory understanding of TTT.
This blog post has nothing to say about
making the idea work in practice,
or why it should do any better than all the other
sub-quadratic sequence models out there.
In my opinion,
the original paper is profound in its philosophy and technical achievements.
I highly recommend giving it a thorough read,
if you haven't already.
Sections 2.1-2.3 are most directly comparable
and they are excellent reads despite this post.
</p>



<h2>acknowledgements</h2>

<p>
This post was inspired by discussions with Krish Parikh and Marcel Roed.
Thanks to Yu Sun and Himanshu Singh for proofreading and feedback.
</p>

<br>
<br>
<br>
<br>
<br>
<br>
<br>

</body>
</html>
