<!DOCTYPE html>
<html>

<head>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="../style.css">
<title>TODO</title>

</head>

<body>
<script type="text/javascript">
MathJax.Hub.Config({
	"HTML-CSS": { scale: 100},
});
</script>

<h1>test-time training as a drop-in replacement for causal self-attention</h1>

<a href="https://arxiv.org/abs/2407.04620"><i>Test-time training (TTT)</i></a>
is a recent approach to sequence modeling
which scales linearly with the sequence length.
Sun et. al.[^ttt] present the TTT layer with both
<i>outer-loop parameters</i>, learnt at train-time,
and <i>inner-loop parameters</i>, learnt at test-time.
In this blog post,
I give an alternate formulation
without outer-loop parameters.
In particular, I show that the TTT computation
is compatible with the parameter-free attention operator.
This suggests that TTT models can be initialized
with pre-trained transformer weights,
which is not particularly highlighted in the paper.
Further, some might find this presentation
easier to digest than the original.

First, I motivate TTT as a substitute for
attention with a single query vector.
Then, I show how TTT achieves linear complexity
in the sequence length by considering
the full causal self-attention operator.

<h2>single-query attention</h2>

Single-query attention has the type signature \( o = a(q, K, V) \), where,
<ul>
<li>\( q \in \mathbb{R}^{d_k} \) is the <i>query</i> vector,</li>
<li>\( K = [k_1, \dots, k_T], k_i \in \mathbb{R}^{d_k}, \) is the list of <i>key</i> vectors,</li>
<li>\( V = [v_1, \dots, v_T], v_i \in \mathbb{R}^{d_v}, \) is the list of <i>value</i> vectors, and,</li>
<li>\( o \in \mathbb{R}^{d_v} \) is the <i>output</i> vector.</li>
</ul>


The standard <i>scaled-dot-product-attention</i> (\( a^{\text{SDPA}} \)) is computed as follows:
<ol>
<li>\( w = \text{softmax}\left(\left\{\frac{q \cdot k_1}{\sqrt{d_k}}, \dots, \frac{q \cdot k_T}{\sqrt{d_k}}\right\}\right) \)</li>
<li>\( o = \sum_{i=1}^t w_i v_i \)</li>
</ol>

Below is an intuitive breakdown of the two steps:
<ol>
<li>Soft-select a key \( k_i \) most <i>similar</i> to the query \( q \),</li>
<li>Return the associated value \( v_i \).</li>
</ol>


This is reminiscent of applying a <i>nearest-neighbor classifier</i> with
train set \( \{(k_1, v_1), \dots, (k_T, v_T)\}, \) and
test input \( q \).
<b>So long as we have a classifier on the key-value pairs,
why not use a neural network?</b>
This would look like below:
<ol>
<li>Train a neural network \( f_\theta \) to predict \( v_i \) given input \( k_i \), and,</li>
<li>Return the prediction of the network on \( q \).</li>
</ol>


This motivates \( a^{\text{TTT}} \), computed as follows:
<ol>
<li>\( \theta = \arg \min_\theta \sum_{i=1}^T \Vert f_\theta(k_i) - v_i \Vert^2 \)</li>
<li>\( o = f_\theta(q) \). </li>
</ol>

Step 1 corresponds to training the network with an\( L_2 \)loss.
If we have to train on all key-value pairs,
then \( a^{\text{TTT}} \) is at least linear in the sequence length,
no better than \( a^{\text{SDPA}} \).
To see how TTT can be more efficient,
we need to consider how single-query attention
is invoked inside the over-arching self-attention computation.


<h2>causal self-attention</h2>

In <i>self-attention</i>, there is a query vector,
and an associated output vector,
for each key-value pair.
The type signature is\( O = A(Q, K, V)\), where,
<ul>
<li>\( Q = [q_1, \dots, q_T]\),\( q_i \in \mathbb{R}^{d_k} \)is the _list of_ query vectors,</li>
<li>\( K = [k_1, \dots, k_T]\),\( k_i \in \mathbb{R}^{d_k} \)is the list of key vectors,</li>
<li>\( V = [v_1, \dots, v_T]\),\( v_i \in \mathbb{R}^{d_v} \)is the list of value vectors, and,</li>
<li>\( O = [o_1, \dots, o_T]\),\( o_i \in \mathbb{R}^{d_v} \)is the _list of_ output vectors.</li>
</ul>


The relation to single-query attention is given by:
\(o_t = a(q_t, [k_1, \dots, k_t], [v_1, \dots, v_t])\)
The dependence of\( o_t \)on\( (k_i, v_i) \)only for\( i \le t \)makes this kind of self-attention <i>causal</i>.

\(A^{\text{SDPA}} \)is quadratic in the sequence length,
as each\( o_t \)is linear in\( t\),
and no further optimization is possible.
\(A^{\text{TTT}} \)escapes this quadratic dependence
by reusing the learnt neural network from the previous timestep,
instead of training a fresh one from scratch.
For example, if we use one step of gradient descent
on every pair\( (k_t, v_t) \)with learning rate\( \eta\),
\(A^{\text{TTT}} \)can be computed as below:

For\( t=1, \dots, T\):
<ol>
<li>\( \theta_t = \theta_{t-1} - \eta \nabla_{\theta_t} \Vert f_{\theta_t}(k_t) - v_t \Vert^2\)</li>
<li>\( o_t = f_\theta(q_t)\)</li>
</ol>

This is linear in the sequence length\( T\).


<h2>epilogue</h2>

What I have presented is only one way to
gain an introductory understanding of TTT.
This blog post has nothing to say about
making the idea work in practice,
or why it should do any better than all the other
sub-quadratic sequence models out there.
In my opinion,
<b>the original paper is profound in its philosophy and technical achievements.</b>
I highly recommend giving it a thorough read,
if you haven't already!
Sections 2.1-2.3 are most directly comparable
and they are excellent reads despite this post.



<h2>acknowledgements</h2>

This post was inspired by discussions with Krish Parikh and Marcel Roed,
and indirectly from Yu Sun's many talks on the topic.

[^ttt]: Learning to (Learn at Test Time): RNNs with Expressive Hidden States, <https://arxiv.org/abs/2407.04620>


</body>
</html>
