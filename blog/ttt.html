<!DOCTYPE html>
<html>

<head>

<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="../style.css">
<title>test-time training as a drop-in replacement for causally masked self-attention</title>

</head>

<body>
<script type="text/javascript">
MathJax.Hub.Config({
	"HTML-CSS": { scale: 100},
});
</script>

<h3>
<a href="../blog.html">blog</a>
| <a href="../index.html">about</a>
</h3>
<br>

<h1>learning to (learn at test time): transformers without quadratic attention</h1>
<h3>November 10, 2024</h3>
<br>

The recent paper
<a href="https://arxiv.org/abs/2407.04620">
"Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
</a>
presents an exciting new approach to sequence modeling,
called <i>test-time training (TTT)</i>.
It scales linearly with the sequence length,
while achieving performance competitive with transformers.
The original paper presents TTT models as "RNNs with expressive hidden states".
In this blog post,
I motivate TTT models from the other direction,
as "transformers without quadratic attention".
Another difference from the original paper is that
in my formulation, the TTT layer does not have any outer-loop parameters,
and so can be a drop-in replacement for the attention module.
One consequence of this perspective is that
TTT models can be initialized with pre-trained transformer weights,
which I find to be me one of the cooler aspects of the idea.




<p>
<i>Test-time training (TTT)</i>,
as described in the paper
<a href="https://arxiv.org/abs/2407.04620">
"Learning to (Learn at Test Time): RNNs with Expressive Hidden States"
</a>,
is an exciting new approach to sequence modeling.
In this blog post,
I motivate TTT as an approximation to the attention mechanism.
This is easier to explain in the case of a single query vector,

First, I have
which scales linearly with the sequence length.
The paper presents the <i>TTT layer</i> with both
<i>outer-loop parameters</i>, learnt at train-time,
and <i>inner-loop parameters</i>, learnt at test-time.
In this blog post,
I give an alternate formulation of the TTT layer
without outer-loop parameters.
Under this formulation,
TTT can be thought of as an approximation to the attention mechanism.
This suggests that TTT models can be initialized
with pre-trained transformer weights,
which is not particularly highlighted in the paper.
I hope that this post adds to the reader's intuitions
about TTT.
</p>

<p>
First, I motivate TTT as a substitute for
an attention call involving a single query vector.
However, this does not immediately lead to linear complexity.
To see that, I consider causal self-attention,
(RR: causally masked self-attention?)
comprising multiple attention calls,
one for each token in the sequence.
The efficiency of TTT arises from reusing the inner-loop parameters
across these multiple calls.
(RR: inner-loop doesn't make sense at this point)
</p>

<h2>approximating attention with test-time training</h2>

Single-query attention has the type signature \( o = a(q, K, V) \), where,
<ul>
<li>\( q \in \mathbb{R}^{d_k} \) is the <i>query</i> vector,</li>
<li>\( K = [k_1, \dots, k_T], k_i \in \mathbb{R}^{d_k}, \) is the list of <i>key</i> vectors,</li>
<li>\( V = [v_1, \dots, v_T], v_i \in \mathbb{R}^{d_v}, \) is the list of <i>value</i> vectors, and,</li>
<li>\( o \in \mathbb{R}^{d_v} \) is the <i>output</i> vector.</li>
</ul>


The standard <i>scaled-dot-product-attention</i> (\( a^{\text{SDPA}} \)) is computed as follows:
<ol>
<li>\( w = \text{softmax}\left(\left\{\frac{q \cdot k_1}{\sqrt{d_k}}, \dots, \frac{q \cdot k_T}{\sqrt{d_k}}\right\}\right) \)</li>
<li>\( o = \sum_{i=1}^t w_i v_i \)</li>
</ol>

Below is an intuitive breakdown of the two steps:
<ol>
<li>Soft-select a key \( k_i \) most <i>similar</i> to the query \( q \),</li>
<li>Return the associated value \( v_i \).</li>
</ol>


This is reminiscent of applying a <i>nearest-neighbor classifier</i> with
train set \( \{(k_1, v_1), \dots, (k_T, v_T)\}, \) and
test input \( q \).
<b>So long as we have a classifier on the key-value pairs,
why not use a neural network?</b>
This would look like below:
<ol>
<li>Train a neural network \( f_\theta \) to predict \( v_i \) given input \( k_i \), and,</li>
<li>Return the prediction of the network on \( q \).</li>
</ol>


This motivates \( a^{\text{TTT}} \), computed as follows:
<ol>
<li>\( \theta = \arg \min_\theta \sum_{i=1}^T \Vert f_\theta(k_i) - v_i \Vert^2 \)</li>
<li>\( o = f_\theta(q) \). </li>
</ol>

Step 1 corresponds to training the network with an\( L_2 \)loss.
If we have to train on all key-value pairs,
then \( a^{\text{TTT}} \) is at least linear in the sequence length,
no better than \( a^{\text{SDPA}} \).
To see how TTT can be more efficient,
we need to consider how single-query attention
is invoked inside the over-arching self-attention computation.


<h2>how does TTT achieve linear scaling?</h2>

In <i>self-attention</i>, there is a query vector,
and an associated output vector,
for each key-value pair.
The type signature is\( O = A(Q, K, V)\), where,
<ul>
<li>\( Q = [q_1, \dots, q_T]\),\( q_i \in \mathbb{R}^{d_k} \)is the _list of_ query vectors,</li>
<li>\( K = [k_1, \dots, k_T]\),\( k_i \in \mathbb{R}^{d_k} \)is the list of key vectors,</li>
<li>\( V = [v_1, \dots, v_T]\),\( v_i \in \mathbb{R}^{d_v} \)is the list of value vectors, and,</li>
<li>\( O = [o_1, \dots, o_T]\),\( o_i \in \mathbb{R}^{d_v} \)is the _list of_ output vectors.</li>
</ul>


The relation to single-query attention is given by:
\(o_t = a(q_t, [k_1, \dots, k_t], [v_1, \dots, v_t])\)
The dependence of\( o_t \)on\( (k_i, v_i) \)only for\( i \le t \)makes this kind of self-attention <i>causal</i>.

\(A^{\text{SDPA}} \)is quadratic in the sequence length,
as each\( o_t \)is linear in\( t\),
and no further optimization is possible.
\(A^{\text{TTT}} \)escapes this quadratic dependence
by reusing the learnt neural network from the previous timestep,
instead of training a fresh one from scratch.
For example, if we use one step of gradient descent
on every pair\( (k_t, v_t) \)with learning rate\( \eta\),
\(A^{\text{TTT}} \)can be computed as below:

For\( t=1, \dots, T\):
<ol>
<li>\( \theta_t = \theta_{t-1} - \eta \nabla_{\theta_t} \Vert f_{\theta_t}(k_t) - v_t \Vert^2\)</li>
<li>\( o_t = f_\theta(q_t)\)</li>
</ol>

This is linear in the sequence length\( T\).

<h2>consequences and comparison with the original paper</h2>

<h2>epilogue</h2>

What I have presented is only one way to
gain an introductory understanding of TTT.
This blog post has nothing to say about
making the idea work in practice,
or why it should do any better than all the other
sub-quadratic sequence models out there.
In my opinion,
<b>the original paper is profound in its philosophy and technical achievements.</b>
I highly recommend giving it a thorough read,
if you haven't already!
Sections 2.1-2.3 are most directly comparable
and they are excellent reads despite this post.



<h2>acknowledgements</h2>

This post was inspired by discussions with Krish Parikh and Marcel Roed,
and indirectly from Yu Sun's many talks on the topic.

[^ttt]: Learning to (Learn at Test Time): RNNs with Expressive Hidden States, <https://arxiv.org/abs/2407.04620>

<br>
<br>
<br>

</body>
</html>
